{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "## import tiktoken\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "block_size = 256      ## max content length for predictions\n",
    "batch_size = 64 \n",
    "max_iters  = 1750\n",
    "eval_interval = 250\n",
    "learning_rate = 3e-4             ## 0.001\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "vocab_size = 65\n",
    "n_embd  = 384                  ## every id gets embedded to vector of this size\n",
    "n_head  = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data_url = 'https://raw.githubusercontent.com/ArchanGhosh/Robert-Frost-Collection-A-Kaggle-Dataset/main/robert_frost_collection.csv'\n",
    "raw_data = pd.read_csv('robert_frost_collection.csv').iloc[1:, :2]\n",
    "\n",
    "with open('input.txt', 'w', encoding='utf-8') as f:\n",
    "    for poem in raw_data.iloc:\n",
    "        f.write(poem.iloc[0] + '\\n' + poem.iloc[1] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = 'input.txt'\n",
    "\n",
    "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"'(),-.123458:;>?ABCDEFGHIJKLMNOPQRSTUVWY_abcdefghijklmnopqrstuvwxyz­·æèê–—‘’“”…\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = { ch:i for i, ch in enumerate(chars) }\n",
    "itos = { i:ch for i, ch in enumerate(chars) }\n",
    "\n",
    "encode = lambda s: [ stoi[c]          for c in s   ]    ## encoder: string to integer\n",
    "decode = lambda l: ''.join(   itos[i] for i in l   )    ## decoder: interger to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(   encode(text), dtype=torch.long   )\n",
    "n    = int(   0.9*len(data)   )\n",
    "train_data = data[:n]\n",
    "val_data   = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    if split == \"train\":\n",
    "        data = train_data\n",
    "    else:\n",
    "        data = val_data\n",
    "    ix = torch.randint(   len(data) - block_size, (batch_size,)   )\n",
    "    x  = torch.stack(    [  data[ i : i+block_size ]   for i in ix]    ) \n",
    "    y  = torch.stack(    [  data[ i+1 : i+1+block_size ]   for i in ix]    )\n",
    "    \n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()    ## for efficiency\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()   ## no training\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()  ## back to training\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key   = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        \n",
    "        ## the mask tril is not part of the graph since only for masking\n",
    "        ## so register buffer makes it a thing out of the graph\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)              ## (B, T, C)\n",
    "        q = self.query(x)            ## (B, T, C)\n",
    "        \n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5       ## (B, T, C) @ (B, C, T)  -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))     ## (B, T, T)\n",
    "        wei = F.softmax(wei, dim= -1)           ## (B, T, T)\n",
    "        wei = self.dropout(   wei   )\n",
    "        \n",
    "        ## perform the weighted aggregation of the values\n",
    "        v   = self.value(  x  )   ## (B, T, C)\n",
    "        out = wei @ v             ## (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(  [Head(head_size) for _ in range(num_heads) ] )\n",
    "        self.proj  = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.cat(   [ h(x) for h in self.heads], dim = -1   )\n",
    "        out = self.proj(  out   )\n",
    "        out = self.dropout(   out   )\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: comuunication followed by computation \"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa   = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward( n_embd)\n",
    "        self.ln1  = nn.LayerNorm(n_embd)\n",
    "        self.ln2  = nn.LayerNorm(n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## these normalizations (ln1, ln2) are about the only thing different from\n",
    "        ## the original Vaswani paper. In the paper, they are done at the end of forward\n",
    "        ## but now they are usually done at the beginning of forward\n",
    "        x = x + self.sa(     self.ln1(x)      )\n",
    "        x = x + self.ffwd(   self.ln2(x)      )\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)     ## positional encoding \n",
    "        self.blocks = nn.Sequential(\n",
    "                *[   Block(n_embd, n_head=n_head) for _ in range(n_layer)    ]\n",
    "        )\n",
    "        self.ln_f    = nn.LayerNorm(  n_embd    )        ## final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        \n",
    "        B, T = idx.shape\n",
    "        \n",
    "        ## ids and targets are both (B, T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx)      ## batch, time, embed (4, 8, 32) \n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))      ## (T, C)\n",
    "        x = tok_emb + pos_emb    ## (B, T, C)\n",
    "        x = self.blocks(  x  )   ## (B, T, C)        \n",
    "        x = self.ln_f(x)         ## (B, T, C)\n",
    "        logits = self.lm_head(x)                 ## (B, T, vocab_sice)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits  = logits.view(B*T, C)\n",
    "            targets  = targets.view(B*T)\n",
    "            loss   = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \n",
    "        ## idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            ## crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            ## get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            ## focus only on last time stamp\n",
    "            logits = logits[:, -1, :]           ## becomes (B, C)\n",
    "            ## apply softmax to get probs\n",
    "            probs = F.softmax(logits, dim= -1)    ## (B, C)\n",
    "            ## sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)     ## (B, 1)\n",
    "            ## append sample to the running sequence\n",
    "            idx = torch.cat(  (idx, idx_next), dim=1  )            ## (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model   = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(  m.parameters(), lr=learning_rate   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.5599, val loss 4.5662\n",
      "step 250: train loss 2.3548, val loss 2.3840\n",
      "step 500: train loss 2.0070, val loss 2.0666\n",
      "step 750: train loss 1.7192, val loss 1.8273\n",
      "step 1000: train loss 1.5528, val loss 1.7139\n",
      "step 1250: train loss 1.4263, val loss 1.6676\n",
      "step 1500: train loss 1.3081, val loss 1.6462\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    ## evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)   ## zero out\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(m, '1750Steps.pkl')\n",
    "m = torch.load('1750Steps.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The shound one way say her the vergen \n",
      "And saved poil. Oh, then way and thin \n",
      "Of song more such all actime \n",
      "That an beared him waite tove. He know, \n",
      "For but that was where lanter whipperal. \n",
      "He mad, didn’t go might him give me wonk \n",
      "The hay place of down it, like andow\n",
      "And child grape awll. \n",
      "A what was lady fraished a made breath lady, \n",
      "And then a firt’s, \n",
      "Of year, and broubtle-righted with.\n",
      "You can set my gob, not man\n",
      "God they or should you ask them fereholly that.\n",
      "Ohink, and mady we havingry, \n"
     ]
    }
   ],
   "source": [
    "## Kick off generation with some starting token. In this case id 0\n",
    "\n",
    "context = torch.zeros(  (1, 1),  dtype=torch.long, device=device   )\n",
    "\n",
    "gen_text = m.generate(context, max_new_tokens=500)[0].tolist()\n",
    "\n",
    "print(  decode(gen_text)   )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2816c529d37cd6f072476ae02d8f4bbad83057dd1e1e56cd5cf923c572082e49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
